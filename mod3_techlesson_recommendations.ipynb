{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886c7ebf",
   "metadata": {},
   "source": [
    "In this lesson, you will learn how to build a recommendation system using the Surprise library in Python. Recommendation systems are algorithms designed to suggest relevant items to users based on available information about users, items, and their interactions. They have become essential in many online platforms including e-commerce (Amazon), streaming services (Netflix, Spotify), and social media (Facebook, Twitter).\n",
    "\n",
    "We'll follow a comprehensive step-by-step process to:\n",
    "\n",
    "1. Load and prepare data.\n",
    "2. Train recommendation models using different algorithms.\n",
    "3. Tune a recommendation model using gridsearch.\n",
    "4. Make predictions for specific users and items.\n",
    "5. Evaluate the models' performance using appropriate metrics.\n",
    "6. Implement a function to generate personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31a9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from surprise import Dataset  # For loading and handling datasets\n",
    "from surprise import Reader   # For parsing custom datasets\n",
    "from surprise import SVD      # Singular Value Decomposition algorithm\n",
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore  # K-Nearest Neighbors algorithms\n",
    "from surprise import NMF      # Non-negative Matrix Factorization algorithm\n",
    "from surprise import BaselineOnly  # Basic algorithm using baselines\n",
    "from surprise.model_selection import train_test_split  # For splitting data\n",
    "from surprise.model_selection import cross_validate    # For cross-validation\n",
    "from surprise.model_selection import GridSearchCV      # For hyperparameter tuning\n",
    "from surprise import accuracy  # For computing prediction accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbe474",
   "metadata": {},
   "source": [
    "#### Step 1: Loading the Data ####\n",
    "\n",
    "For this lesson, we'll use the Surprise built-in MovieLens 100k dataset, which contains 100,000 ratings (1-5) from 943 users on 1,682 movies. This is a popular benchmark dataset for recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9953e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the built-in MovieLens dataset\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a locally saved dataset\n",
    "\n",
    "# Define the format of your custom file\n",
    "file_path = 'your_ratings.csv'\n",
    "\n",
    "# Create a reader object specifying the rating scale\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale=(1, 5))\n",
    "\n",
    "# Load the data from your file\n",
    "data = Dataset.load_from_file(file_path, reader)\n",
    "\n",
    "# If your data is in a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame structure\n",
    "df = pd.DataFrame({\n",
    "    'user': [1, 1, 2, 2],\n",
    "    'item': [101, 102, 101, 103],\n",
    "    'rating': [4, 3, 5, 2]\n",
    "})\n",
    "\n",
    "# Create a reader object\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load data from DataFrame using column names\n",
    "data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d18c7",
   "metadata": {},
   "source": [
    "#### Step 2: Splitting the Data ####\n",
    "\n",
    "Surprise uses a special masking procedure to hide known ratings from the data in order to create a train and test set. These masked ratings can then be compared to their predicted ratings from the model in order to evaluate the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054cf1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets (75% training, 25% testing)\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5fc72",
   "metadata": {},
   "source": [
    "#### Step 3: Selecting and Training a Model\n",
    "\n",
    "*Note:* we will use the full data set to cross validate as Surprise train_test_split cannot work with cross validation given its unique method of splitting data. We will use our trainset and testset to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda0395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "\n",
      "Algorithm Comparison:\n",
      "------------------------------------------------------------\n",
      "Algorithm                      RMSE            MAE            \n",
      "------------------------------------------------------------\n",
      "SVD                            0.9372          0.7389         \n",
      "KNNBasic (User-based)          0.9790          0.7732         \n",
      "KNNBasic (Item-based)          0.9738          0.7693         \n",
      "KNNWithMeans (User-based)      0.9508          0.7492         \n",
      "NMF                            0.9661          0.7595         \n",
      "BaselineOnly                   0.9441          0.7483         \n"
     ]
    }
   ],
   "source": [
    "# Define a list of algorithms to compare\n",
    "algorithms = [\n",
    "    SVD(),\n",
    "    KNNBasic(sim_options={'user_based': True}),  # User-based collaborative filtering\n",
    "    KNNBasic(sim_options={'user_based': False}), # Item-based collaborative filtering\n",
    "    KNNWithMeans(sim_options={'user_based': True}),\n",
    "    NMF(),\n",
    "    BaselineOnly()\n",
    "]\n",
    "\n",
    "# Evaluate each algorithm using cross-validation\n",
    "results = {}\n",
    "for algo in algorithms:\n",
    "    algo_name = algo.__class__.__name__\n",
    "    sim_option = ''\n",
    "    \n",
    "    # Add user/item based info for KNN algorithms\n",
    "    if algo_name.startswith('KNN'):\n",
    "        user_based = algo.sim_options.get('user_based', True)\n",
    "        sim_option = 'User-based' if user_based else 'Item-based'\n",
    "        algo_name = f\"{algo_name} ({sim_option})\"\n",
    "    \n",
    "    # Run 5-fold cross-validation\n",
    "    cv_results = cross_validate(algo, data, measures=['RMSE', 'MAE'], \n",
    "                               cv=5, verbose=False)\n",
    "    \n",
    "    # Store results\n",
    "    results[algo_name] = {\n",
    "        'RMSE': cv_results['test_rmse'].mean(),\n",
    "        'MAE': cv_results['test_mae'].mean()\n",
    "    }\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nAlgorithm Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Algorithm':<30} {'RMSE':<15} {'MAE':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for algo_name, metrics in results.items():\n",
    "    print(f\"{algo_name:<30} {metrics['RMSE']:<15.4f} {metrics['MAE']:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d578411",
   "metadata": {},
   "source": [
    "**Algorithm Descriptions**\n",
    "\n",
    "- SVD (Singular Value Decomposition): A matrix factorization technique that decomposes the user-item rating matrix into lower-dimensional user and item factors.\n",
    "- KNNBasic (K-Nearest Neighbors Basic):\n",
    "    - User-based: Predicts ratings based on the ratings of similar users\n",
    "    - Item-based: Predicts ratings based on the user's ratings of similar items\n",
    "- KNNWithMeans: Similar to KNNBasic, but takes into account the mean ratings of each user when calculating similarities.\n",
    "- NMF (Non-negative Matrix Factorization): A matrix factorization method where all factors must be non-negative, which can lead to more interpretable factors.\n",
    "- BaselineOnly: A simple algorithm that predicts the baseline estimate for a given user and item, which consists of global average, user bias, and item bias.\n",
    "\n",
    "**Algorithm Strengths and Weaknesses**\n",
    "\n",
    "- SVD typically provides good accuracy and can handle large datasets efficiently.\n",
    "- KNN methods are more interpretable (recommendations can be explained by showing similar users/items).\n",
    "- NMF might provide more interpretable latent factors than SVD.\n",
    "- BaselineOnly serves as a good baseline to compare against more sophisticated algorithms.\n",
    "\n",
    "The best algorithm depends on the specific dataset and application. Comparing multiple algorithms allows us to select the one that performs best for our particular use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae724f",
   "metadata": {},
   "source": [
    "#### Step 4: Hyperparameter Tuning ####\n",
    "\n",
    "The SVD model had the lowest RMSE and MAE so we will proceed with that model as the one to tweak via hyperparameter tuning. \n",
    "\n",
    "Key Hyperparameters:\n",
    "\n",
    "*n_factors*: Number of latent factors (the dimensionality of the user and item vectors).\n",
    "- More factors increase ability to capture patterns but increase computational cost and may lead to overfitting\n",
    "\n",
    "*n_epochs*: Number of iterations of SGD (Stochastic Gradient Descent).\n",
    "- More epochs may lead to better convergence but increase the risk of overfitting.\n",
    "\n",
    "*lr_all*: Learning rate for all parameters.\n",
    "- Lower learning rates may result in more stable convergence but require more epochs.\n",
    "\n",
    "*reg_all*: Regularization term for all parameters.\n",
    "- Higher regularization values help prevent overfitting but might reduce the model's ability to capture patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f364d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RMSE Parameters:\n",
      "{'n_factors': 150, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 42}\n",
      "Best RMSE Score: 0.9123\n",
      "\n",
      "Best MAE Parameters:\n",
      "{'n_factors': 150, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1, 'random_state': 42}\n",
      "Best MAE Score: 0.7215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x20cdb0aeca0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid for SVD\n",
    "param_grid = {\n",
    "   'n_factors': [50, 100, 150],\n",
    "   'n_epochs': [10, 20, 30],\n",
    "   'lr_all': [0.002, 0.005, 0.01],\n",
    "   'reg_all': [0.02, 0.1, 0.5],\n",
    "   'random_state': [42]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "gs.fit(data)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest RMSE Parameters:\")\n",
    "print(gs.best_params['rmse'])\n",
    "print(f\"Best RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "\n",
    "print(\"\\nBest MAE Parameters:\")\n",
    "print(gs.best_params['mae'])\n",
    "print(f\"Best MAE Score: {gs.best_score['mae']:.4f}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_algo = SVD(**gs.best_params['mae'])\n",
    "best_algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35e779",
   "metadata": {},
   "source": [
    "#### Step 5: Make Predictions and Evaluate ####\n",
    "\n",
    "Each prediction contains:\n",
    "\n",
    "- uid: The user identifier\n",
    "- iid: The item identifier\n",
    "- r_ui: The true rating given by the user\n",
    "- est: The estimated rating predicted by the algorithm\n",
    "- The difference between true and estimated ratings indicates the prediction error\n",
    "\n",
    "The goal is to minimize this error across all predictions. A smaller error indicates a better-performing recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8edc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 391, Item: 591, Actual Rating: 4.00, Predicted Rating: 3.53, Error: 0.47\n",
      "User: 181, Item: 1291, Actual Rating: 1.00, Predicted Rating: 1.54, Error: -0.54\n",
      "User: 637, Item: 268, Actual Rating: 2.00, Predicted Rating: 2.80, Error: -0.80\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testset\n",
    "predictions = best_algo.test(testset)\n",
    "\n",
    "# Look at the first few predictions\n",
    "for pred in predictions[:3]:\n",
    "    print(f\"User: {pred.uid}, Item: {pred.iid}, \"\n",
    "          f\"Actual Rating: {pred.r_ui:.2f}, Predicted Rating: {pred.est:.2f}, \"\n",
    "          f\"Error: {pred.r_ui - pred.est:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456a0124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9176\n",
      "MAE:  0.7244\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE and MAE on the testset\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051da23",
   "metadata": {},
   "source": [
    "On average this recommendation model's rating predictions are off by about 0.72 of a rating level. We could continue to tune the model given the results of our first gridsearch to try and improve further, but for the sake of time and this lesson we will stop here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c042de",
   "metadata": {},
   "source": [
    "#### Step 6: Make Specific Recommendations ####\n",
    "\n",
    "To provide practical recommendations, we want to suggest items a user hasn't interacted with yet which are predicted to be highly rated by that user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b22f6dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 movie recommendations for user 196:\n",
      "Movie ID: 408, Predicted Rating: 4.57\n",
      "Movie ID: 318, Predicted Rating: 4.54\n",
      "Movie ID: 169, Predicted Rating: 4.54\n",
      "Movie ID: 483, Predicted Rating: 4.52\n",
      "Movie ID: 64, Predicted Rating: 4.52\n"
     ]
    }
   ],
   "source": [
    "# Create a full training set that includes all users\n",
    "full_trainset = data.build_full_trainset()\n",
    "\n",
    "# Fit model to all users and data\n",
    "best_algo.fit(full_trainset)\n",
    "\n",
    "def get_top_n_recommendations(algo, data, user_id, n=10):\n",
    "    \"\"\"\n",
    "    Generate top-N recommendations for a specific user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    algo : surprise.prediction_algorithms\n",
    "        Trained algorithm\n",
    "    data : surprise.Trainset\n",
    "        The full training dataset\n",
    "    user_id : str\n",
    "        The user ID for whom to generate recommendations\n",
    "    n : int, default=10\n",
    "        Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        (item_id, predicted_rating) sorted by predicted rating in descending order\n",
    "    \"\"\"\n",
    "    # Get a list of all items\n",
    "    all_item_ids = data.all_items()\n",
    "    \n",
    "    # Convert raw user ID to inner ID used by the trainset\n",
    "    try:\n",
    "        inner_user_id = data.to_inner_uid(user_id)\n",
    "    except ValueError:\n",
    "        print(f\"User {user_id} doesn't exist in the training set\")\n",
    "        return []\n",
    "    \n",
    "    # Get items rated by this user\n",
    "    user_items = [j for (j, _) in data.ur[inner_user_id]]\n",
    "    \n",
    "    # Items not rated by the user\n",
    "    unrated_items = [item_id for item_id in all_item_ids if item_id not in user_items]\n",
    "    \n",
    "    # Predict ratings for unrated items\n",
    "    predictions = []\n",
    "    for item_id in unrated_items:\n",
    "        # Convert inner item ID back to raw ID for prediction\n",
    "        raw_item_id = data.to_raw_iid(item_id)\n",
    "        # Get prediction\n",
    "        pred = algo.predict(user_id, raw_item_id)\n",
    "        predictions.append((raw_item_id, pred.est))\n",
    "    \n",
    "    # Sort predictions by estimated rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top n recommendations\n",
    "    return predictions[:n]\n",
    "\n",
    "# Example usage\n",
    "user_id = '196'  # Choose a user ID from the dataset\n",
    "top_recommendations = get_top_n_recommendations(algo, trainset, user_id, n=5)\n",
    "\n",
    "print(f\"Top 5 movie recommendations for user {user_id}:\")\n",
    "for movie_id, predicted_rating in top_recommendations:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacad89",
   "metadata": {},
   "source": [
    "This function works by:\n",
    "\n",
    "1. Identifying all items not yet rated by the user.\n",
    "2. Predicting how the user would rate each unrated item.\n",
    "3. Sorting these items by their predicted ratings.\n",
    "4. Returning the top-N items with the highest predicted ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6443de",
   "metadata": {},
   "source": [
    "This approach ensures we're recommending new items the user hasn't seen yet, rather than items they've already rated. The conversion between inner IDs (used by Surprise internally) and raw IDs (the original identifiers) is a crucial step when working with the Surprise library. The returned Movie IDâ€™s can then be connected with meta data to provide contextual information about the movie (title, genre, etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
