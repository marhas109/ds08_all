{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN with Distance Metrics - Customer Segmentation Lab:\n",
    "\n",
    "## Business Scenario\n",
    "\n",
    "You work as a data scientist for RetailIQ, an e-commerce analytics company that helps online retailers better understand their customers. One of your clients, an online fashion retailer, wants to segment their customer base to create more targeted marketing campaigns.\n",
    "\n",
    "Your task is to develop a customer segmentation model using k-Nearest Neighbors with various distance metrics. The client has provided data on customer purchasing behavior, demographics, and engagement metrics.\n",
    "\n",
    "They've already identified five customer segments in their previous marketing research:\n",
    "- Segment 0: Occasional Shoppers (low frequency, low value)\n",
    "- Segment 1: Loyal Regular Shoppers (high frequency, moderate value)\n",
    "- Segment 2: High-Value Enthusiasts (high frequency, high value)\n",
    "- Segment 3: Big Spenders (low frequency, high value)\n",
    "- Segment 4: New Customers (recent first purchase)\n",
    "\n",
    "The goal is to build a model that can accurately classify new customers into these segments based on their behavior and attributes, so that marketing strategies can be personalized for each segment.\n",
    "\n",
    "## The Process\n",
    "\n",
    "By the end of this lab, you will have:\n",
    "1. Analyzed the dataset to understand the characteristics of customer features\n",
    "2. Preprocessed the data appropriately for distance calculations\n",
    "3. Implemented k-NN with different distance metrics\n",
    "4. Evaluated and compared the performance of each metric\n",
    "5. Tuned and optimized the best-performing model\n",
    "6. Evaluated the performance of the final model and best distance metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup - Import Libraries and Load Data\n",
    "\n",
    "First, let's import all the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step0\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and exploring the dataset\n",
    "\n",
    "The dataset contains customer information and a target variable 'segment' indicating their assigned segment (0-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step0\n",
    "# Load the dataset\n",
    "customer_data = pd.read_csv('retail_customer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Display basic information about the dataset\n",
    "customer_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check the first few rows of the dataset\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check the distribution of the target variable\n",
    "customer_data['segment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check basic statistics of the dataset\n",
    "customer_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analyzing Dataset Characteristics\n",
    "\n",
    "Before selecting a distance metric, we need to understand the characteristics of our data. Let's look at feature distributions and correlations.\n",
    "\n",
    "### Feature Distributions\n",
    "Analyzing feature distributions will help us understand if we need to standardize our data before applying distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step1\n",
    "# Create histograms of all features to observe their distributions\n",
    "# Select all numeric columns except the target\n",
    "numeric_columns = None\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(15, 10))\n",
    "customer_data[numeric_columns].hist(bins=20, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# Create a correlation matrix to identify feature relationships - use the full dataframe including segment\n",
    "correlation_matrix = None\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Characteristics Analysis\n",
    "\n",
    "Based on the histograms and correlation matrix above, consider how these questions help you:\n",
    "\n",
    "1. Do you observe any features with significantly different scales? What impact would this have on distance calculations?\n",
    "\n",
    "2. Are there correlations between features? Which distance metric might be more appropriate for correlated features?\n",
    "\n",
    "3. Based on your analysis, which preprocessing steps would you recommend before applying k-NN with distance metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing\n",
    "\n",
    "Now, let's prepare our data for k-NN modeling, applying the preprocessing steps you identified as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step2\n",
    "# Prepare features (X) and target (y)\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = None\n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes to display results\n",
    "# Display a comparison of original vs. scaled data for the first sample\n",
    "print(\"Original first sample:\")\n",
    "print(X_train.iloc[0].values[:5], \"...\")\n",
    "print(\"\\nScaled first sample:\")\n",
    "print(X_train_scaled[0][:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing k-NN with Different Distance Metrics\n",
    "\n",
    "Now, let's implement k-NN with various distance metrics and evaluate their performance using cross-validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step3\n",
    "# Distance metrics to test\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "k_value = 5\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Loop through list of metrics\n",
    "None:\n",
    "    # Create and evaluate model with different metrics and k=5\n",
    "    knn = None\n",
    "    # Get cross val scores for model\n",
    "    cv_scores = None\n",
    "    # Store the mean of cv scores as value and metric name as key in results dictionary\n",
    "    None\n",
    "    \n",
    "best_metric = max(results, key=results.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Find the best metric\n",
    "print(results)\n",
    "print(f\"\\nBest metric: {best_metric} with accuracy: {results[best_metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Implementing Mahalanobis Distance\n",
    "\n",
    "Mahalanobis distance is particularly useful for datasets with correlated features. Let's implement it separately and compare its performance to the other metrics we tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step4\n",
    "# Calculate covariance matrix from the training data\n",
    "cov = None\n",
    "\n",
    "# Implement k-NN with Mahalanobis distance and k=5\n",
    "knn_mahalanobis = None\n",
    "\n",
    "# Evaluate performance via cross validation\n",
    "cv_scores_mahalanobis = None\n",
    "results['mahalanobis'] = cv_scores_mahalanobis.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "print(f\"Average CV accuracy with Mahalanobis: {cv_scores_mahalanobis.mean():.4f}\")\n",
    "\n",
    "# Update best metric if necessary\n",
    "if results['mahalanobis'] > results[best_metric]:\n",
    "    best_metric = 'mahalanobis'\n",
    "    print(f\"New best metric: {best_metric} with accuracy: {results[best_metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Hyperparameter Tuning\n",
    "\n",
    "Now, let's optimize our model by finding the best k value and weighting scheme for the top-performing distance metric.\n",
    "\n",
    "Use the following information for your grid search:\n",
    "- 'n_neighbors': [1, 3, 5, 7, 9]\n",
    "- 'weights': ['uniform', 'distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step5\n",
    "# Define parameter grid\n",
    "param_grid = None\n",
    "\n",
    "# Create base model with best metric\n",
    "base_model = None\n",
    "\n",
    "# Initialize and run grid search\n",
    "grid_search = None \n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best parameters and accuracy\n",
    "best_params = None\n",
    "best_cv_accuracy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Visualization of accuracy for different k values\n",
    "# This helps us understand the relationship between k and model performance\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validation accuracy: {best_cv_accuracy:.4f}\")\n",
    "\n",
    "k_range = range(1, 31)\n",
    "k_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of k')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.title(f'Accuracy for Different Values of k using {best_metric}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Final Model Evaluation\n",
    "\n",
    "Let's build our final model with the optimized parameters and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step6\n",
    "# Build final model with best parameters\n",
    "final_model = None\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = None\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = None\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cohort_Env)",
   "language": "python",
   "name": "cohort_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
